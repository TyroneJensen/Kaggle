{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-18T14:32:33.058583Z","iopub.execute_input":"2023-01-18T14:32:33.058923Z","iopub.status.idle":"2023-01-18T14:32:33.089593Z","shell.execute_reply.started":"2023-01-18T14:32:33.058845Z","shell.execute_reply":"2023-01-18T14:32:33.088476Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Genetic Hyperparameter Tuning with TPOT\nYou're going to undertake a simple example of genetic hyperparameter tuning. TPOT is a very powerful library that has a lot of features. You're just scratching the surface in this lesson, but you are highly encouraged to explore in your own time.\n\nThis is a very small example. In real life, TPOT is designed to be run for many hours to find the best model. You would have a much larger population and offspring size as well as hundreds more generations to find a good model.\n\nYou will create the estimator, fit the estimator to the training data and then score this on the test data.\n\nFor this example we wish to use:\n\n- 3 generations\n- 4 in the population size\n- 3 offspring in each generation\n- accuracy for scoring\n\nA random_state of 2 has been set for consistency of results.","metadata":{}},{"cell_type":"markdown","source":"#### TPOT components\n\nThe key arguments to a TPOT classifier are:\n- generations: Iterations to run training for.\n- population_size: The number of models to keep after each iteration.\n- offspring_size: Number of models to produce in each iteration.\n- mutation_rate: The proportion of pipelines to apply randomness to.\n- crossover_rate: The proportion of pipelines to breed each iteration.\n- scoring: The function to determine the best models.\n- cv: Cross-validation strategy to use.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\n\nX = data.drop('default.payment.next.month', axis=1)\ny = data['default.payment.next.month']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:32:37.528847Z","iopub.execute_input":"2023-01-18T14:32:37.530386Z","iopub.status.idle":"2023-01-18T14:32:38.159384Z","shell.execute_reply.started":"2023-01-18T14:32:37.530339Z","shell.execute_reply":"2023-01-18T14:32:38.158310Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:32:48.723404Z","iopub.execute_input":"2023-01-18T14:32:48.723884Z","iopub.status.idle":"2023-01-18T14:32:48.735734Z","shell.execute_reply.started":"2023-01-18T14:32:48.723850Z","shell.execute_reply":"2023-01-18T14:32:48.733885Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(20100, 24)"},"metadata":{}}]},{"cell_type":"code","source":"from tpot import TPOTClassifier\n\n# Assign the values outlined to the inputs\nnumber_generations = 10\npopulation_size = 10\noffspring_size = None\nscoring_function = 'accuracy'\n\n# Create the tpot classifier\ntpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n                          offspring_size=offspring_size, scoring=scoring_function,\n                          verbosity=2, random_state=2, cv=2)\n\n# Fit the classifier to the training data\ntpot_clf.fit(X_train, y_train)\n\n# Score on the test set\nprint(tpot_clf.score(X_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:32:53.530277Z","iopub.execute_input":"2023-01-18T14:32:53.530719Z","iopub.status.idle":"2023-01-18T14:50:56.449512Z","shell.execute_reply.started":"2023-01-18T14:32:53.530685Z","shell.execute_reply":"2023-01-18T14:50:56.448210Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Optimization Progress:   0%|          | 0/110 [00:00<?, ?pipeline/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb8306ba9a14ac7b9f9a2fb1295eb90"}},"metadata":{}},{"name":"stdout","text":"\nGeneration 1 - Current best internal CV score: 0.8183084577114428\n\nGeneration 2 - Current best internal CV score: 0.8186567164179105\n\nGeneration 3 - Current best internal CV score: 0.8192537313432836\n\nGeneration 4 - Current best internal CV score: 0.8192537313432836\n\nGeneration 5 - Current best internal CV score: 0.8192537313432836\n\nGeneration 6 - Current best internal CV score: 0.8192537313432836\n\nGeneration 7 - Current best internal CV score: 0.8192537313432836\n\nGeneration 8 - Current best internal CV score: 0.8192537313432836\n\nGeneration 9 - Current best internal CV score: 0.8200497512437811\n\nGeneration 10 - Current best internal CV score: 0.8200497512437811\n\nBest pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.7500000000000001, min_samples_leaf=18, min_samples_split=15, n_estimators=100)\n0.8218181818181818\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Nice work! You can see in the output the score produced by the chosen model over each generation, and then the final accuracy score with the hyperparameters chosen for the final model. This is a great first example of using TPOT for automated hyperparameter tuning. You can now extend on this on your own and build great machine learning models!","metadata":{}},{"cell_type":"code","source":"# NOTE: tweak random state param","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well done! You can see that TPOT is quite unstable when only running with low generations, population size and offspring. The first model chosen was a Decision Tree, then a K-nearest Neighbor model and finally a Random Forest. Increasing the generations, population size and offspring and running this for a long time will assist to produce better models and more stable results. Don't hesitate to try it yourself on your own machine!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}