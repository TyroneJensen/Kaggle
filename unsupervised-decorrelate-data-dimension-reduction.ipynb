{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-21T09:19:10.505944Z","iopub.execute_input":"2022-06-21T09:19:10.506356Z","iopub.status.idle":"2022-06-21T09:19:10.515913Z","shell.execute_reply.started":"2022-06-21T09:19:10.506322Z","shell.execute_reply":"2022-06-21T09:19:10.514875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlated data**\n- Pearson correlation measures linear correlation of features\n- Value btw -1 and 1\n- Value of 0 means no linear correlatioin","metadata":{}},{"cell_type":"code","source":"# using grains dataset make a scatter plot of width vs length and measure their Pearson correlation.\n\ngrains_df = pd.read_csv('../input/seeds-grains-data-set/grains_2.csv')\ngrains_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:10.528832Z","iopub.execute_input":"2022-06-21T09:19:10.529188Z","iopub.status.idle":"2022-06-21T09:19:10.547945Z","shell.execute_reply.started":"2022-06-21T09:19:10.529156Z","shell.execute_reply":"2022-06-21T09:19:10.547286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grains_df = grains_df.drop('Class', axis=1)\n\ngrains_df2 = grains_df[['width','length']]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:10.551696Z","iopub.execute_input":"2022-06-21T09:19:10.552204Z","iopub.status.idle":"2022-06-21T09:19:10.558905Z","shell.execute_reply.started":"2022-06-21T09:19:10.55216Z","shell.execute_reply":"2022-06-21T09:19:10.558017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform the necessary imports \nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\n# assign columns\nwidth = grains_df['width']\nlength = grains_df['length']\n\n# Scatter plot width vs length\nplt.figure(figsize=(6,6))\nplt.scatter(width, length)\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation\ncorrelation, pvalue = pearsonr(width, length)\n\n# Display the correlation\nprint(f'Pearson correlation: {correlation}')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:10.5677Z","iopub.execute_input":"2022-06-21T09:19:10.568107Z","iopub.status.idle":"2022-06-21T09:19:10.71612Z","shell.execute_reply.started":"2022-06-21T09:19:10.568066Z","shell.execute_reply":"2022-06-21T09:19:10.715187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decorrelating the grain measurements with PCA**","metadata":{}},{"cell_type":"code","source":"# use PCA to decorrelate width and length measurements, then plot the decorrelated points and measure their Pearson correlation\n\n# Import PCA\nfrom sklearn.decomposition import PCA\n\n# Create PCA instance: model\nmodel = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model.fit_transform(grains_df)\n\n# Assign 3rd column of pca_features: xs\nxs = pca_features[:,3]\n\n# Assign 4th column of pca_features: ys\nys = pca_features[:,4]\n\n# Scatter plot xs vs ys\nplt.scatter(xs, ys)\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation of xs and ys\ncorrelation, pvalue = pearsonr(xs, ys)\n\n# Display the correlation\nprint(correlation)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:10.717936Z","iopub.execute_input":"2022-06-21T09:19:10.718536Z","iopub.status.idle":"2022-06-21T09:19:10.852106Z","shell.execute_reply.started":"2022-06-21T09:19:10.718494Z","shell.execute_reply":"2022-06-21T09:19:10.851265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot**","metadata":{}},{"cell_type":"code","source":"# Make a scatter plot of the untransformed points\nplt.scatter(width,length)\n\n# Create a PCA instance: model\nmodel = PCA()\n\n# Fit model to points\nmodel.fit(grains_df2)\n\n# Get the mean of the grain samples: mean\nmean = model.mean_\n\n# Get the first principal component: first_pc\nfirst_pc = model.components_[0,:]\n\n# Plot first_pc as an arrow, starting at mean\nplt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n\n# Keep axes on same scale\nplt.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:10.853692Z","iopub.execute_input":"2022-06-21T09:19:10.854105Z","iopub.status.idle":"2022-06-21T09:19:10.990763Z","shell.execute_reply.started":"2022-06-21T09:19:10.854064Z","shell.execute_reply":"2022-06-21T09:19:10.989687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Variance of the PCA features**\n- The fish dataset is 6-dimensional. But what is its intrinsic dimension? \n- Make a plot of the variances of the PCA features to find out. \n- You'll need to standardize the features first.","metadata":{}},{"cell_type":"code","source":"fish_df = pd.read_csv('../input/fish-measurements-dataset/fish_measurements.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:10.993108Z","iopub.execute_input":"2022-06-21T09:19:10.993564Z","iopub.status.idle":"2022-06-21T09:19:11.002651Z","shell.execute_reply.started":"2022-06-21T09:19:10.993523Z","shell.execute_reply":"2022-06-21T09:19:11.001727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples = fish_df.drop('species', axis =1)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.004475Z","iopub.execute_input":"2022-06-21T09:19:11.005187Z","iopub.status.idle":"2022-06-21T09:19:11.011855Z","shell.execute_reply.started":"2022-06-21T09:19:11.005142Z","shell.execute_reply":"2022-06-21T09:19:11.010806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline '\npipeline.fit(samples)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.013348Z","iopub.execute_input":"2022-06-21T09:19:11.014386Z","iopub.status.idle":"2022-06-21T09:19:11.144844Z","shell.execute_reply.started":"2022-06-21T09:19:11.014341Z","shell.execute_reply":"2022-06-21T09:19:11.144148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like PCA features 0 and 1 have significant variance, so the intrinsic dimension of the dataset appears to be 2","metadata":{}},{"cell_type":"code","source":"# Create a PCA model with 2 components identified above\npca = PCA(n_components=2)\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline '\npipeline.fit(samples)\n\n# Transform the scaled samples: pca_features\npca_features = pipeline.transform(samples)\n\n# Print the shape of pca_features\nprint(pca_features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.146048Z","iopub.execute_input":"2022-06-21T09:19:11.146827Z","iopub.status.idle":"2022-06-21T09:19:11.159154Z","shell.execute_reply.started":"2022-06-21T09:19:11.146785Z","shell.execute_reply":"2022-06-21T09:19:11.157984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now have reduced the dimensionality from 6 to 2","metadata":{}},{"cell_type":"markdown","source":"**tf-idf word-frequency array**\n- ***TfidfVectorize*** transforms a list of documents into a word frequency array, which it outputs as a csr_matrix\n- csr_matrix is a sparse array (remembers only the non-zero entries (saves space!))\n- scikit-learn PCA doesn't support csr_matrix, use scikit-learn TruncatedSVD instead (Performs same transformation)\n","metadata":{}},{"cell_type":"code","source":"# create list of documents\ndocuments = ['cats say meow', 'dogs say woof', 'dogs chase cats']\n\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create a TfidfVectorizer: tfidf\ntfidf = TfidfVectorizer() \n\n# Apply fit_transform to document: csr_mat\ncsr_mat = tfidf.fit_transform(documents)\n\n# Print result of toarray() method\nprint(csr_mat.toarray())\n\n# Get the words: words\nwords = tfidf.get_feature_names_out()\n\n# Print words\nprint(words)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.160851Z","iopub.execute_input":"2022-06-21T09:19:11.161467Z","iopub.status.idle":"2022-06-21T09:19:11.177892Z","shell.execute_reply.started":"2022-06-21T09:19:11.161426Z","shell.execute_reply":"2022-06-21T09:19:11.17709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clustering Wikipedia**\n- TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays\n- using TruncatedSVD and k-means to cluster some popular pages from Wikipedia\n\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/wikipedia-articles-vector-file/wikipedia-vectors.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.179583Z","iopub.execute_input":"2022-06-21T09:19:11.180273Z","iopub.status.idle":"2022-06-21T09:19:11.261317Z","shell.execute_reply.started":"2022-06-21T09:19:11.180206Z","shell.execute_reply":"2022-06-21T09:19:11.260349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles = df.drop('Unnamed: 0', axis =1)\narticles.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.263075Z","iopub.execute_input":"2022-06-21T09:19:11.263378Z","iopub.status.idle":"2022-06-21T09:19:11.293278Z","shell.execute_reply.started":"2022-06-21T09:19:11.263352Z","shell.execute_reply":"2022-06-21T09:19:11.29254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titles = articles.columns.values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.296786Z","iopub.execute_input":"2022-06-21T09:19:11.297321Z","iopub.status.idle":"2022-06-21T09:19:11.302314Z","shell.execute_reply.started":"2022-06-21T09:19:11.297276Z","shell.execute_reply":"2022-06-21T09:19:11.301533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform the necessary imports\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\n\n# Create a TruncatedSVD instance: svd\nsvd = TruncatedSVD(n_components=50)\n\n# Create a KMeans instance: kmeans\nkmeans = KMeans(n_clusters=6)\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(svd, kmeans)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.304783Z","iopub.execute_input":"2022-06-21T09:19:11.305323Z","iopub.status.idle":"2022-06-21T09:19:11.31346Z","shell.execute_reply.started":"2022-06-21T09:19:11.305292Z","shell.execute_reply":"2022-06-21T09:19:11.312729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.314602Z","iopub.execute_input":"2022-06-21T09:19:11.315031Z","iopub.status.idle":"2022-06-21T09:19:11.324882Z","shell.execute_reply.started":"2022-06-21T09:19:11.314999Z","shell.execute_reply":"2022-06-21T09:19:11.324291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transpose dataframe\narticles = articles.T","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.325981Z","iopub.execute_input":"2022-06-21T09:19:11.326466Z","iopub.status.idle":"2022-06-21T09:19:11.335214Z","shell.execute_reply.started":"2022-06-21T09:19:11.326436Z","shell.execute_reply":"2022-06-21T09:19:11.334519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the pipeline to articles\npipeline.fit(articles)\n\n# Calculate the cluster labels: labels\nlabels = pipeline.predict(articles)\n\n# Create a DataFrame aligning labels and titles: df\ndf = pd.DataFrame({'label': labels, 'article': titles})\n\n# Display df sorted by cluster label\nprint(df.sort_values('label'))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T09:19:11.336353Z","iopub.execute_input":"2022-06-21T09:19:11.33682Z","iopub.status.idle":"2022-06-21T09:19:12.026691Z","shell.execute_reply.started":"2022-06-21T09:19:11.336781Z","shell.execute_reply":"2022-06-21T09:19:12.025575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at the cluster labels and see if you can identify any patterns","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}